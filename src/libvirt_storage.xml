<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
         xmlns:its="http://www.w3.org/2005/11/its"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         version="5.2"
         xml:id="cha-libvirt-storage">
   <title>Advanced storage topics</title>
   <info/>
   <para>This chapter introduces advanced topics about manipulating storage from the perspective of the VM Host Server.</para>
   <sect1 xml:id="sec-libvirt-storage-locking">
      <title>Locking disk files and block devices with <systemitem class="daemon">virtlockd</systemitem>
      </title>
      <para>Locking block devices and disk files prevents concurrent writes to these resources from different VM Guests. It provides protection against starting the same VM Guest twice, or adding the same disk to two different virtual machines. This reduces the risk of a virtual machine's disk image becoming corrupted because of a wrong configuration.</para>
      <para>The locking is controlled by a daemon called <systemitem class="daemon">virtlockd</systemitem>. Since it operates independently from the <systemitem class="daemon">libvirtd</systemitem> daemon, locks endure a crash or a restart of <systemitem class="daemon">libvirtd</systemitem>. Locks even persist during an update of the <systemitem class="daemon">virtlockd</systemitem> itself, since it can re-execute itself. This ensures that VM Guests do <emphasis>not</emphasis> need to be restarted upon a <systemitem class="daemon">virtlockd</systemitem> update. <systemitem class="daemon">virtlockd</systemitem> is supported for KVM, QEMU, and Xen.</para>
      <sect2 xml:id="sec-libvirt-storage-locking-enable">
         <title>Enable locking</title>
         <para>Locking virtual disks is not enabled by default on <phrase role="productname">SUSE Linux Enterprise Server</phrase>. To enable and automatically start it upon rebooting, perform the following steps:</para>
         <procedure>
            <step>
               <para>Edit <filename>/etc/libvirt/qemu.conf</filename> and set</para>
               <screen>lock_manager = "lockd"</screen>
            </step>
            <step>
               <para>Start the <systemitem class="daemon">virtlockd</systemitem> daemon with the following command:</para>
               <screen><prompt>&gt;</prompt><command>sudo</command> systemctl start virtlockd</screen>
            </step>
            <step>
               <para>Restart the <systemitem class="daemon">libvirtd</systemitem> daemon with:</para>
               <screen><prompt>&gt;</prompt><command>sudo</command> systemctl restart libvirtd</screen>
            </step>
            <step>
               <para>Make sure <systemitem class="daemon">virtlockd</systemitem> is automatically started when booting the system:</para>
               <screen><prompt>&gt;</prompt><command>sudo</command> systemctl enable virtlockd</screen>
            </step>
         </procedure>
      </sect2>
      <sect2 xml:id="sec-libvirt-storage-locking-configure">
         <title>Configure locking</title>
         <para>By default <systemitem class="daemon">virtlockd</systemitem> is configured to automatically lock all disks configured for your VM Guests. The default setting uses a <quote>direct</quote> lockspace, where the locks are acquired against the actual file paths associated with the VM Guest &lt;disk&gt; devices. For example, <literal>flock(2)</literal> is called directly on <filename>/var/lib/libvirt/images/my-server/disk0.raw</filename> when the VM Guest contains the following &lt;disk&gt; device:</para>
         <screen>&lt;disk type='file' device='disk'&gt;
 &lt;driver name='qemu' type='raw'/&gt;
 &lt;source file='/var/lib/libvirt/images/my-server/disk0.raw'/&gt;
 &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
         <para>The <systemitem class="daemon">virtlockd</systemitem> configuration can be changed by editing the file <filename>/etc/libvirt/qemu-lockd.conf</filename>. It also contains detailed comments with further information. Make sure to activate configuration changes by reloading <systemitem class="daemon">virtlockd</systemitem>:</para>
         <screen><prompt>&gt;</prompt><command>sudo</command> systemctl reload virtlockd</screen>
         <!-- fs 2014-08-05: FIXME Check if tru &sle; 11 SP3
        <note>
        <title>Locking currently only available for all disks</title>
        <para>
        Currently, locking can only be activated globally, so that all virtual
        disks are locked. Support for locking selected disks is planned for future
        releases.
        </para>
        </note>
        <sect3 id="sec-libvirt-storage-locking-configure-noauto">
        <title>Manually managing locks</title>
        <para>
        As stated above, all virtual disks are locked by default. To
        restrict locking to selected disks, you need to turn off auto locking by
        setting
        </para>
        <screen>auto_disk_leases = 0</screen>
        <para>
        If auto locking is turned off you need to add &lt;lease> statements to
        the &lt;devices> section of the &vmguest;s XML definitions by editing
        them with the <command>virsh edit</command> command. A sample entry will
        look like the following example:
        </para>
        see https://www.redhat.com/archives/libvir-list/2013-April/msg01714.html
        <screen>TBD</screen>
        <para>
        Device leases
        When using a lock manager, it may be desirable to record device leases against a VM. The lock manager will ensure the VM won't start unless the leases can be acquired.
        ...
        <devices>
        ...
        <lease>
        <lockspace>somearea</lockspace>
        <key>somekey</key>
        <target path='/some/lease/path' offset='1024'/>
        </lease>
        ...
        </devices>
        ...
        lockspace
        This is an arbitrary string, identifying the lockspace within which the key is held. Lock managers may impose extra restrictions on the format, or length of the lockspace name.
        key
        This is an arbitrary string, uniquely identifying the lease to be acquired. Lock managers may impose extra restrictions on the format, or length of the key.
        target
        This is the fully qualified path of the file associated with the
        lockspace. The offset specifies where the lease is stored within the file. If
        the lock manager does not require a offset, just pass 0.
        </para>
        <important>
        <title>Starting &vmguest;s without locking</title>
        <para>
        If auto locking is disabled, <systemitem
        class="daemon">virtlockd</systemitem> automatically prevents all
        &vmguest;s that have no proper <quote>lease-configuration</quote> from
        being started. This ensures that only &vmguest;s with disks that are
        locked can be started.
        </para>
        <para>
        Disable this behavior by setting
        </para>
        <screen>require_lease_for_disks = 0</screen>
        </important>
        </sect3>
        -->
         <sect3 xml:id="sec-libvirt-storage-locking-configure-shared-fs">
            <title>Enabling an indirect lockspace</title>
            <para>The default configuration of <systemitem class="daemon">virtlockd</systemitem> uses a <quote>direct</quote> lockspace. This means that the locks are acquired against the actual file paths associated with the &lt;disk&gt; devices.</para>
            <para>If the disk file paths are not accessible to all hosts, <systemitem class="daemon">virtlockd</systemitem> can be configured to allow an <quote>indirect</quote> lockspace. This means that a hash of the disk image path is used to create a file in the indirect lockspace directory. The locks are then held on these hash files instead of the actual disk file paths. Indirect lockspace is also useful if the file system containing the disk files does not support <literal>fcntl()</literal> locks. An indirect lockspace is specified with the <option>file_lockspace_dir</option> setting:</para>
            <screen>file_lockspace_dir = "<replaceable>/MY_LOCKSPACE_DIRECTORY</replaceable>"</screen>
         </sect3>
         <sect3 xml:id="sec-libvirt-storage-locking-configure-lvm-iscsi">
            <title>Enable locking on LVM or iSCSI volumes</title>
            <para>When wanting to lock virtual disks placed on LVM or iSCSI volumes shared by several hosts, locking needs to be done by UUID rather than by path (which is used by default). Furthermore, the lockspace directory needs to be placed on a shared file system accessible by all hosts sharing the volume. Set the following options for LVM and/or iSCSI:</para>
            <screen>lvm_lockspace_dir = "<replaceable>/MY_LOCKSPACE_DIRECTORY</replaceable>"
iscsi_lockspace_dir = "<replaceable>/MY_LOCKSPACE_DIRECTORY</replaceable>"</screen>
         </sect3>
      </sect2>
   </sect1>
   <sect1 xml:id="sec-libvirt-storage-resize">
      <title>Online resizing of guest block devices</title>
      <para>Sometimes you need to change—extend or shrink—the size of the block device used by your guest system. For example, when the disk space originally allocated is no longer enough, it is time to increase its size. If the guest disk resides on a <emphasis>logical volume</emphasis>, you can resize it while the guest system is running. This is a big advantage over an offline disk resizing (see the <command>virt-resize</command> command from the <xref linkend="sec-guestfs-tools"/> package) as the service provided by the guest is not interrupted by the resizing process. To resize a VM Guest disk, follow these steps:</para>
      <procedure>
         <title>Online resizing of guest disk</title>
         <step>
            <para>Inside the guest system, check the current size of the disk (for example <filename>/dev/vda</filename>).</para>
            <screen><prompt role="root">#</prompt>fdisk -l /dev/vda
Disk /dev/sda: 160.0 GB, 160041885696 bytes, 312581808 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes</screen>
         </step>
         <step>
            <para>On the host, resize the logical volume holding the <filename>/dev/vda</filename> disk of the guest to the required size, for example, 200 GB.</para>
            <screen><prompt role="root">#</prompt>lvresize -L 200G /dev/mapper/vg00-home
Extending logical volume home to 200 GiB
Logical volume home successfully resized</screen>
         </step>
         <step>
            <para>On the host, resize the block device related to the disk <filename>/dev/mapper/vg00-home</filename> of the guest. You can find the <replaceable>DOMAIN_ID</replaceable> with <command>virsh list</command>.</para>
            <screen><prompt role="root">#</prompt>virsh blockresize  --path /dev/vg00/home --size 200G <replaceable>DOMAIN_ID</replaceable>
Block device '/dev/vg00/home' is resized</screen>
         </step>
         <step>
            <para>Check that the new disk size is accepted by the guest.</para>
            <screen><prompt role="root">#</prompt>fdisk -l /dev/vda
Disk /dev/sda: 200.0 GB, 200052357120 bytes, 390727260 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes</screen>
         </step>
      </procedure>
   </sect1>
   <sect1 xml:id="sec-libvirt-storage-share">
      <title>Sharing directories between host and guests (file system pass-through)</title>
      <para>libvirt allows to share directories between host and guests using QEMU's file system pass-through (also called VirtFS) feature. Such a directory can be also be accessed by several VM Guests at once and therefore be used to exchange files between VM Guests.</para>
      <note>
         <title>Windows guests and file system pass-through</title>
         <para>Sharing directories between VM Host Server and Windows guests via File System Pass-Through does not work, because Windows lacks the drivers required to mount the shared directory.</para>
      </note>
      <para>To make a shared directory available on a VM Guest, proceed as follows:</para>
      <procedure>
         <step>
            <para>Open the guest's console in Virtual Machine Manager and either choose <menuchoice><guimenu>View</guimenu><guimenu>Details</guimenu></menuchoice> from the menu or click <guimenu>Show virtual hardware details</guimenu> in the toolbar. Choose <menuchoice><guimenu>Add Hardware</guimenu><guimenu>Filesystem</guimenu></menuchoice> to open the <guimenu>Filesystem Passthrough</guimenu> dialog.</para>
         </step>
         <step>
            <para><guimenu>Driver</guimenu> allows you to choose between a <guimenu>Handle</guimenu> or <guimenu>Path</guimenu> base driver. The default setting is <guimenu>Path</guimenu>. <guimenu>Mode</guimenu> lets you choose the security model, which influences the way file permissions are set on the host. Three options are available:</para>
            <variablelist>
               <varlistentry>
                  <term>
                     <guimenu>Passthrough</guimenu> (default)</term>
                  <listitem>
                     <para>Files on the file system are directly created with the client-user's credentials. This is similar to what NFSv3 is using.</para>
                  </listitem>
               </varlistentry>
               <varlistentry>
                  <term>
                     <guimenu>Squash</guimenu>
                  </term>
                  <listitem>
                     <para>Same as <guimenu>Passthrough</guimenu>, but failure of privileged operations like <command>chown</command> are ignored. This is required when KVM is not run with <systemitem class="username">root</systemitem> privileges.</para>
                  </listitem>
               </varlistentry>
               <varlistentry>
                  <term>
                     <guimenu>Mapped</guimenu>
                  </term>
                  <listitem>
                     <para>Files are created with the file server's credentials (<literal>qemu.qemu</literal>). The user credentials and the client-user's credentials are saved in extended attributes. This model is recommended when host and guest domains should be kept isolated.</para>
                  </listitem>
               </varlistentry>
            </variablelist>
         </step>
         <step>
            <para>Specify the path to the directory on the VM Host Server with <guimenu>Source Path</guimenu>. Enter a string at <guimenu>Target Path</guimenu> to be used as a tag to mount the shared directory. The string of this field is a tag only, not a path on the VM Guest.</para>
         </step>
         <step>
            <para><guimenu>Apply</guimenu> the setting. If the VM Guest is currently running, you need to shut it down to apply the new setting (rebooting the guest is not sufficient).</para>
         </step>
         <step>
            <para>Boot the VM Guest. To mount the shared directory, enter the following command:</para>
            <screen><prompt>&gt;</prompt><command>sudo</command> mount -t 9p -o trans=virtio,version=9p2000.L,rw <replaceable>TAG</replaceable> /<replaceable>MOUNT_POINT</replaceable></screen>
            <para>To make the shared directory permanently available, add the following line to the <filename>/etc/fstab</filename> file:</para>
            <screen><replaceable>TAG</replaceable>   /<replaceable>MOUNT_POINT</replaceable>    9p  trans=virtio,version=9p2000.L,rw    0   0</screen>
         </step>
      </procedure>
   </sect1>
   <sect1 xml:id="libvirt-storage-rbd">
      <title>Using RADOS block devices with <systemitem class="library">libvirt</systemitem>
      </title>
      <para>RADOS Block Devices (RBD) store data in a Ceph cluster. They allow snapshotting, replication and data consistency. You can use an RBD from your <systemitem class="library">libvirt</systemitem>-managed VM Guests similarly to how you use other block devices.</para>
      <para>For more details, refer to the SUSE Enterprise Storage <citetitle>Administration Guide</citetitle>, chapter <citetitle>Using libvirt with Ceph</citetitle>. The SUSE Enterprise Storage documentation is available from <link xlink:href="https://documentation.suse.com/ses/"/>.</para>
   </sect1>
</chapter>
