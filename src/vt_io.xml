<?xml version="1.0" encoding="UTF-8"?>
<sect1 xmlns="http://docbook.org/ns/docbook"
       xmlns:its="http://www.w3.org/2005/11/its"
       xmlns:xi="http://www.w3.org/2001/XInclude"
       xmlns:xlink="http://www.w3.org/1999/xlink"
       version="5.2"
       xml:id="sec-vt-io">
   <title>I/O virtualization</title>
   <info/>
   <para>VM Guests not only share CPU and memory resources of the host system, but also the I/O subsystem. Because software I/O virtualization techniques deliver less performance than bare metal, hardware solutions that deliver almost <quote>native</quote> performance have been developed recently. <phrase role="productname">SUSE Linux Enterprise Server</phrase> supports the following I/O virtualization techniques:</para>
   <variablelist>
      <varlistentry xml:id="vt-io-fullv">
         <term>Full virtualization</term>
         <listitem>
            <para>Fully Virtualized (FV) drivers emulate widely supported real devices, which can be used with an existing driver in the VM Guest. The guest is also called <emphasis>Hardware Virtual Machine</emphasis> (HVM). Since the physical device on the VM Host Server may differ from the emulated one, the hypervisor needs to process all I/O operations before handing them over to the physical device. Therefore all I/O operations need to traverse two software layers, a process that not only significantly impacts I/O performance, but also consumes CPU time.</para>
         </listitem>
      </varlistentry>
      <varlistentry xml:id="vt-io-parav">
         <term>Paravirtualization</term>
         <listitem>
            <para>Paravirtualization (PV) allows direct communication between the hypervisor and the VM Guest. With less overhead involved, performance is much better than with full virtualization. However, paravirtualization requires either the guest operating system to be modified to support the paravirtualization API or paravirtualized drivers. See <xref linkend="sec-kvm-requires-guests-virt-drivers"/> for a list of guest operating systems supporting paravirtualization.</para>
         </listitem>
      </varlistentry>
      <varlistentry xml:id="vt-io-pvhvm">
         <term>PVHVM</term>
         <listitem>
            <para>This type of virtualization enhances HVM (see <xref linkend="vt-io-fullv"/>) with paravirtualized (PV) drivers, and PV interrupt and timer handling.</para>
         </listitem>
      </varlistentry>
      <varlistentry xml:id="vt-io-vfio">
         <term>VFIO</term>
         <listitem>
            <para>VFIO stands for <emphasis>Virtual Function I/O</emphasis> and is a new user-level driver framework for Linux. It replaces the traditional KVM PCI Pass-Through device assignment. The VFIO driver exposes direct device access to user space in a secure memory (<xref linkend="gloss-vt-acronym-iommu"/>) protected environment. With VFIO, a VM Guest can directly access hardware devices on the VM Host Server (pass-through), avoiding performance issues caused by emulation in performance critical paths. This method does not allow to share devices—each device can only be assigned to a single VM Guest. VFIO needs to be supported by the VM Host Server CPU, chipset and the BIOS/EFI.</para>
            <para>Compared to the legacy KVM PCI device assignment, VFIO has the following advantages:</para>
            <itemizedlist mark="bullet" spacing="normal">
               <listitem>
                  <para>Resource access is compatible with UEFI Secure Boot.</para>
               </listitem>
               <listitem>
                  <para>Device is isolated and its memory access protected.</para>
               </listitem>
               <listitem>
                  <para>Offers a user space device driver with more flexible device ownership model.</para>
               </listitem>
               <listitem>
                  <para>Is independent of KVM technology, and not bound to x86 architecture only.</para>
               </listitem>
            </itemizedlist>
            <para>In <phrase role="productname">SUSE Linux Enterprise Server</phrase> the USB and PCI pass-through methods of device assignment are considered deprecated and are superseded by the VFIO model.</para>
         </listitem>
      </varlistentry>
      <varlistentry xml:id="vt-io-sriov">
         <term>SR-IOV</term>
         <listitem>
            <para>The latest I/O virtualization technique, Single Root I/O Virtualization SR-IOV combines the benefits of the aforementioned techniques—performance and the ability to share a device with several VM Guests. SR-IOV requires special I/O devices, that are capable of replicating resources so they appear as multiple separate devices. Each such <quote>pseudo</quote> device can be directly used by a single guest. However, for network cards for example the number of concurrent queues that can be used is limited, potentially reducing performance for the VM Guest compared to paravirtualized drivers. On the VM Host Server, SR-IOV must be supported by the I/O device, the CPU and chipset, the BIOS/EFI and the hypervisor—for setup instructions see <xref linkend="sec-libvirt-config-pci"/>. <!-- fs 2014-02-21: no list available
     ATM &productname; supports the SRV-IOV capable network cards listed below
     --></para>
         </listitem>
      </varlistentry>
   </variablelist>
   <important xml:id="ann-vt-io-require">
      <title>Requirements for VFIO and SR-IOV</title>
      <para>To be able to use the VFIO and SR-IOV features, the VM Host Server needs to fulfill the following requirements:</para>
      <itemizedlist>
         <listitem>
            <para>IOMMU needs to be enabled in the BIOS/EFI.</para>
         </listitem>
         <listitem>
            <para>For Intel CPUs, the kernel parameter <literal>intel_iommu=on</literal> needs to be provided on the kernel command line. For more information, see <link xlink:href="https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/kernel-parameters.txt#L1951"/>.</para>
         </listitem>
         <listitem>
            <para>The VFIO infrastructure needs to be available. This can be achieved by loading the kernel module <systemitem class="resource">vfio_pci</systemitem>. For more information, see <phrase xmlns:l="http://docbook.org/ns/docbook/l10n" role="document-xref">Book <quote>Administration Guide</quote>, </phrase><xref linkend="sec-boot-systemd-advanced-kernel-modules" xrefstyle="%label &#34;%c&#34;"/>.</para>
         </listitem>
      </itemizedlist>
   </important>
</sect1>
