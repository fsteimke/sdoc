<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
         xmlns:its="http://www.w3.org/2005/11/its"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         version="5.2"
         xml:id="cha-xen-admin">
   <title>Administrative tasks</title>
   <info/>
   <sect1 xml:id="sec-xen-config-bootloader">
      <title>The boot loader program</title>
      <para>The boot loader controls how the virtualization software boots and runs. You can modify the boot loader properties by using YaST, or by directly editing the boot loader configuration file.</para>
      <para>The YaST boot loader program is located at <menuchoice><guimenu>YaST</guimenu><guimenu>System</guimenu><guimenu>Boot Loader</guimenu></menuchoice>. Click the <guimenu>Bootloader Options</guimenu> tab and select the line containing the Xen kernel as the <guimenu>Default Boot Section</guimenu>.</para>
      <figure>
         <title>Boot loader settings</title>
         <mediaobject>
            <imageobject outputformat="print">
               <imagedata fileref="media/xen_bootloader.png" width="73%"/>
            </imageobject>
            <imageobject outputformat="screen">
               <imagedata fileref="media/xen_bootloader.png" width="73%"/>
            </imageobject>
         </mediaobject>
      </figure>
      <para>Confirm with <guimenu>OK</guimenu>. Next time you boot the host, it can provide the Xen virtualization environment.</para>
      <para>You can use the Boot Loader program to specify functionality, such as:</para>
      <itemizedlist mark="bullet" spacing="normal">
         <listitem>
            <para>Pass kernel command-line parameters.</para>
         </listitem>
         <listitem>
            <para>Specify the kernel image and initial RAM disk.</para>
         </listitem>
         <listitem>
            <para>Select a specific hypervisor.</para>
         </listitem>
         <listitem>
            <para>Pass additional parameters to the hypervisor. See <link xlink:href="https://xenbits.xen.org/docs/unstable/misc/xen-command-line.html"/> for their complete list.</para>
         </listitem>
      </itemizedlist>
      <para>You can customize your virtualization environment by editing the <filename>/etc/default/grub</filename> file. Add the following line to this file: <literal>GRUB_CMDLINE_XEN="&lt;boot_parameters&gt;"</literal>. Do not forget to run <command>grub2-mkconfig -o /boot/grub2/grub.cfg</command> after editing the file.</para>
   </sect1>
   <sect1 xml:id="sec-xen-config-sparse">
      <title>Sparse image files and disk space</title>
      <para>If the host’s physical disk reaches a state where it has no available space, a virtual machine using a virtual disk based on a sparse image file cannot write to its disk. Consequently, it reports I/O errors.</para>
      <para>If this situation occurs, you should free up available space on the physical disk, remount the virtual machine’s file system, and set the file system back to read-write.</para>
      <para>To check the actual disk requirements of a sparse image file, use the command <command>du -h &lt;image file&gt;</command>.</para>
      <para>To increase the available space of a sparse image file, first increase the file size and then the file system.</para>
      <warning>
         <title>Back up before resizing</title>
         <para>Touching the sizes of partitions or sparse files always bears the risk of data failure. Do not work without a backup.</para>
      </warning>
      <para>The resizing of the image file can be done online, while the VM Guest is running. Increase the size of a sparse image file with:</para>
      <screen><prompt>&gt;</prompt><command>sudo</command> dd if=/dev/zero of=&lt;image file&gt; count=0 bs=1M seek=&lt;new size in MB&gt;</screen>
      <para>For example, to increase the file <filename>/var/lib/xen/images/sles/disk0</filename> to a size of 16GB, use the command:</para>
      <screen><prompt>&gt;</prompt><command>sudo</command> dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 count=0 bs=1M seek=16000</screen>
      <note>
         <title>Increasing non-sparse images</title>
         <para>It is also possible to increase the image files of devices that are not sparse files. However, you must know exactly where the previous image ends. Use the seek parameter to point to the end of the image file and use a command similar to the following:</para>
         <screen><prompt>&gt;</prompt><command>sudo</command> dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 seek=8000 bs=1M count=2000</screen>
      </note>
      <para>Be sure to use the right seek, else data loss may happen.</para>
      <para>If the VM Guest is running during the resize operation, also resize the loop device that provides the image file to the VM Guest. First detect the correct loop device with the command:</para>
      <screen><prompt>&gt;</prompt><command>sudo</command> losetup -j /var/lib/xen/images/sles/disk0</screen>
      <para>Then resize the loop device, for example, <filename>/dev/loop0</filename>, with the following command:</para>
      <screen><prompt>&gt;</prompt><command>sudo</command> losetup -c /dev/loop0</screen>
      <para>Finally check the size of the block device inside the guest system with the command <command>fdisk -l /dev/xvdb</command>. Replace the device name with your increased disk.</para>
      <para>The resizing of the file system inside the sparse file involves tools that are depending on the actual file system. This is described in detail in the <xref linkend="book-storage"/>.</para>
   </sect1>
   <sect1 xml:id="sec-xen-manage-migrate">
      <title>Migrating Xen VM Guest systems</title>
      <para>With Xen it is possible to migrate a VM Guest system from one VM Host Server to another with almost no service interruption. This could be used, for example, to move a busy VM Guest to a VM Host Server that has stronger hardware or is not yet loaded. Or, if a service of a VM Host Server is required, all VM Guest systems running on this machine can be migrated to other machines to avoid interruption of service. These are only two examples—many more reasons may apply to your personal situation.</para>
      <para>Before starting, certain preliminary considerations regarding the VM Host Server should be taken into account:</para>
      <itemizedlist mark="bullet" spacing="normal">
         <listitem>
            <para>All VM Host Server systems should use a similar CPU. The frequency is not so important, but they should be using the same CPU family. To get more information about the used CPU, use <command>cat /proc/cpuinfo</command>. Find more details about comparing host CPU features in <xref linkend="sec-xen-manage-migrate-cpu"/>.</para>
         </listitem>
         <listitem>
            <para>All resources that are used by a specific guest system must be available on all involved VM Host Server systems—for example, all used block devices must exist on both VM Host Server systems.</para>
         </listitem>
         <listitem>
            <para>If the hosts included in the migration process run in different subnets, make sure that either DHCP relay is available to the guests, or for guests with static network configuration, set up the network manually.</para>
         </listitem>
         <listitem>
            <para>Using special features like <literal>PCI Pass-Through</literal> may be problematic. Do not implement these when deploying for an environment that should migrate VM Guest systems between different VM Host Server systems.</para>
         </listitem>
         <listitem>
            <para>For fast migrations, a fast network is mandatory. If possible, use GB Ethernet and fast switches. Deploying VLAN may also help avoid collisions.</para>
         </listitem>
      </itemizedlist>
      <sect2 xml:id="sec-xen-manage-migrate-cpu">
         <title>Detecting CPU features</title>
         <para>By using the <systemitem>cpuid</systemitem> and <systemitem>xen_maskcalc.py</systemitem> tools, you can compare features of a CPU on the host from where you are migrating the source VM Guest with the features of CPUs on the target hosts. This way you can better predict if the guest migrations will be successful.</para>
         <procedure>
            <step>
               <para>Run the <command>cpuid -1r</command> command on each Dom0 that is supposed to run or receive the migrated VM Guest and capture the output in text files, for example:</para>
               <screen><prompt>tux@vm_host1 &gt;</prompt>sudo cpuid -1r &gt; vm_host1.txt
<prompt>tux@vm_host2 &gt;</prompt>sudo cpuid -1r &gt; vm_host2.txt
<prompt>tux@vm_host3 &gt;</prompt>sudo cpuid -1r &gt; vm_host3.txt
</screen>
            </step>
            <step>
               <para>Copy all the output text files on a host with the <command>xen_maskcalc.py</command> script installed.</para>
            </step>
            <step>
               <para>Run the <command>xen_maskcalc.py</command> script on all output text files:</para>
               <screen><prompt>&gt;</prompt><command>sudo</command> xen_maskcalc.py vm_host1.txt vm_host2.txt vm_host3.txt
cpuid = [
    "0x00000001:ecx=x00xxxxxx0xxxxxxxxx00xxxxxxxxxxx",
    "0x00000007,0x00:ebx=xxxxxxxxxxxxxxxxxx00x0000x0x0x00"
]
</screen>
            </step>
            <step>
               <para>Copy the output <literal>cpuid=[...]</literal> configuration snipped into the <command>xl</command> configuration of the migrated guest<filename>domU.cfg</filename> or alternatively to its <systemitem class="library">libvirt</systemitem>'s XML configuration.</para>
            </step>
            <step>
               <para>Start the source guest with the <emphasis>trimmed</emphasis> CPU configuration. The guest can now only use CPU features which are present on each of the hosts.</para>
            </step>
         </procedure>
         <tip>
            <para><systemitem class="library">libvirt</systemitem> also supports calculating a baseline CPU for migration. For more details, refer to <phrase xmlns:l="http://docbook.org/ns/docbook/l10n" role="document-xref">Book <quote>Quick Start Guides</quote>, </phrase><xref linkend="article-virtualization-best-practices" xrefstyle="%label &#34;%c&#34;"/>.</para>
         </tip>
         <sect3 xml:id="sec-xen-manage-migrate-cpu-info">
            <title>More information</title>
            <para>You can find more details about <systemitem>cpuid</systemitem> at <link xlink:href="https://etallen.com/cpuid.html"/>.</para>
            <para>You can download the latest version of the CPU mask calculator from <link xlink:href="https://github.com/twizted/xen_maskcalc"/>.</para>
         </sect3>
      </sect2>
      <sect2 xml:id="sec-xen-manage-migrate-block">
         <title>Preparing block devices for migrations</title>
         <para>The block devices needed by the VM Guest system must be available on all involved VM Host Server systems. This is done by implementing a specific kind of shared storage that serves as a container for the root file system of the migrated VM Guest system. Common possibilities include:</para>
         <itemizedlist mark="bullet" spacing="normal">
            <listitem>
               <para><literal>iSCSI</literal> can be set up to give access to the same block devices from different systems at the same time. For more information about iSCSI, see <phrase xmlns:l="http://docbook.org/ns/docbook/l10n" role="document-xref">Book <quote>Storage Administration Guide</quote>, </phrase><xref linkend="cha-iscsi" xrefstyle="%label &#34;%c&#34;"/>.</para>
            </listitem>
            <listitem>
               <para><literal>NFS</literal> is a widely used root file system that can easily be accessed from different locations. For more information, see <phrase xmlns:l="http://docbook.org/ns/docbook/l10n" role="document-xref">Book <quote>Storage Administration Guide</quote>, </phrase><xref linkend="cha-nfs" xrefstyle="%label &#34;%c&#34;"/>.</para>
            </listitem>
            <listitem>
               <para><literal>DRBD</literal> can be used if only two VM Host Server systems are involved. This adds certain extra data security, because the used data is mirrored over the network. For more information, see the <citetitle>SUSE Linux Enterprise High Availability <phrase role="productnumber">15 SP7</phrase></citetitle> documentation at <link xlink:href="https://documentation.suse.com/sle-ha-15/"/>.</para>
            </listitem>
            <listitem>
               <para><literal>SCSI</literal> can also be used if the available hardware permits shared access to the same disks.</para>
            </listitem>
            <listitem>
               <para><literal>NPIV</literal> is a special mode to use Fibre channel disks. However, in this case, all migration hosts must be attached to the same Fibre channel switch. For more information about NPIV, see <xref linkend="sec-xen-config-disk"/>. Commonly, this works if the Fibre channel environment supports 4 Gbps or faster connections.</para>
            </listitem>
         </itemizedlist>
      </sect2>
      <sect2 xml:id="sec-xen-manage-migrate-xl">
         <title>Migrating VM Guest systems</title>
         <para>The actual migration of the VM Guest system is done with the command:</para>
         <screen><prompt>&gt;</prompt><command>sudo</command> xl migrate &lt;domain_name&gt; &lt;host&gt;</screen>
         <para>The speed of the migration depends on how fast the memory print can be saved to disk, sent to the new VM Host Server and loaded there. This means that small VM Guest systems can be migrated faster than big systems with a lot of memory.</para>
      </sect2>
   </sect1>
   <sect1 xml:id="sec-xen-monitor">
      <title>Monitoring Xen</title>
      <para>For a regular operation of many virtual guests, having a possibility to check the sanity of all the different VM Guest systems is indispensable. Xen offers several tools besides the system tools to gather information about the system.</para>
      <tip>
         <title>Monitoring the VM Host Server</title>
         <para>Basic monitoring of the VM Host Server (I/O and CPU) is available via the Virtual Machine Manager. Refer to <xref linkend="cha-libvirt-admin-monitor-virt-manager"/> for details.</para>
      </tip>
      <sect2 xml:id="sec-xen-monitor-xentop">
         <title>Monitor Xen with <command>xentop</command>
         </title>
         <para>The preferred terminal application to gather information about Xen virtual environment is <command>xentop</command>. Be aware that this tool needs a rather broad terminal, else it inserts line breaks into the display.</para>
         <para><command>xentop</command> has several command keys that can give you more information about the system that is monitored. For example:</para>
         <variablelist>
            <varlistentry>
               <term>D</term>
               <listitem>
                  <para>Change the delay between the refreshes of the screen.</para>
               </listitem>
            </varlistentry>
            <varlistentry>
               <term>N</term>
               <listitem>
                  <para>Also display network statistics. Note, that only standard configurations are displayed. If you use a special configuration like a routed network, no network is displayed.</para>
               </listitem>
            </varlistentry>
            <varlistentry>
               <term>B</term>
               <listitem>
                  <para>Display the respective block devices and their cumulated usage count.</para>
               </listitem>
            </varlistentry>
         </variablelist>
         <para>For more information about <command>xentop</command>, see the manual page <command>man 1 xentop</command>.</para>
         <tip>
            <title>
               <command>virt-top</command>
            </title>
            <para>libvirt offers the hypervisor-agnostic tool <command>virt-top</command>, which is recommended for monitoring VM Guests. See <xref linkend="cha-libvirt-admin-monitor-virt-top"/> for details.</para>
         </tip>
      </sect2>
      <sect2 xml:id="sec-xen-monitor-tools">
         <title>Additional tools</title>
         <para>There are many system tools that also help monitoring or debugging a running SUSE Linux Enterprise system. Many of these are covered in <phrase xmlns:l="http://docbook.org/ns/docbook/l10n" role="document-xref">Book <quote>System Analysis and Tuning Guide</quote>, </phrase><xref linkend="cha-util" xrefstyle="%label &#34;%c&#34;"/>. Especially useful for monitoring a virtualization environment are the following tools:</para>
         <variablelist>
            <varlistentry>
               <term>ip</term>
               <listitem>
                  <para>The command-line utility <command>ip</command> may be used to monitor arbitrary network interfaces. This is especially useful if you have set up a network that is routed or applied a masqueraded network. To monitor a network interface with the name <literal>alice.0</literal>, run the following command:</para>
                  <screen><prompt>&gt;</prompt>watch ip -s link show alice.0</screen>
               </listitem>
            </varlistentry>
            <varlistentry>
               <term>bridge</term>
               <listitem>
                  <para>In a standard setup, all the Xen VM Guest systems are attached to a virtual network bridge. <command>bridge</command> allows you to determine the connection between the bridge and the virtual network adapter in the VM Guest system. For example, the output of <command>bridge link</command> may look like the following:</para>
                  <screen>
2: eth0 state DOWN : &lt;NO-CARRIER, ...,UP&gt; mtu 1500 master br0
8: vnet0 state UNKNOWN : &lt;BROADCAST, ...,LOWER_UP&gt; mtu 1500 master virbr0 \
  state forwarding priority 32 cost 100
</screen>
                  <para>This shows that there are two virtual bridges defined on the system. One is connected to the physical Ethernet device <literal>eth0</literal>, the other one is connected to a VLAN interface <literal>vnet0</literal>.</para>
               </listitem>
            </varlistentry>
            <varlistentry>
               <term>iptables-save</term>
               <listitem>
                  <para>Especially when using masquerade networks, or if several Ethernet interfaces are set up together with a firewall setup, it may be helpful to check the current firewall rules.</para>
                  <para>The command <command>iptables</command> may be used to check all the different firewall settings. To list all the rules of a chain, or even of the complete setup, you may use the commands <command>iptables-save</command> or <command>iptables -S</command>.</para>
               </listitem>
            </varlistentry>
         </variablelist>
      </sect2>
   </sect1>
   <sect1 xml:id="sec-xen-admin-vhostmd">
      <title>Providing host information for VM Guest systems</title>
      <para>In a standard Xen environment, the VM Guest systems have only limited information about the VM Host Server system they are running on. If a guest should know more about the VM Host Server it runs on, <systemitem>vhostmd</systemitem> can provide more information to selected guests. To set up your system to run <systemitem>vhostmd</systemitem>, proceed as follows:</para>
      <procedure>
         <step>
            <para>Install the package vhostmd on the VM Host Server.</para>
         </step>
         <step>
            <para>To add or remove <literal>metric</literal> sections from the configuration, edit the file <filename>/etc/vhostmd/vhostmd.conf</filename>. However, the default works well.</para>
         </step>
         <step>
            <para>Check the validity of the <filename>vhostmd.conf</filename> configuration file with the command:</para>
            <screen><prompt>&gt;</prompt>cd /etc/vhostmd
<prompt>&gt;</prompt>xmllint --postvalid --noout vhostmd.conf
    </screen>
         </step>
         <step>
            <para>Start the vhostmd daemon with the command <command>sudo systemctl start vhostmd</command>.</para>
            <para>If vhostmd should be started automatically during start-up of the system, run the command:</para>
            <screen><prompt>&gt;</prompt><command>sudo</command> systemctl enable vhostmd</screen>
         </step>
         <step>
            <para>Attach the image file <filename>/dev/shm/vhostmd0</filename> to the VM Guest system named alice with the command:</para>
            <screen><prompt>&gt;</prompt>xl block-attach opensuse /dev/shm/vhostmd0,,xvdb,ro</screen>
         </step>
         <step>
            <para>Log on the VM Guest system.</para>
         </step>
         <step>
            <para>Install the client package <systemitem>vm-dump-metrics</systemitem>.</para>
         </step>
         <step>
            <para>Run the command <command>vm-dump-metrics</command>. To save the result to a file, use the option <systemitem>-d &lt;filename&gt;</systemitem>.</para>
         </step>
      </procedure>
      <para>The result of the <systemitem>vm-dump-metrics</systemitem> is an XML output. The respective metric entries follow the DTD <filename>/etc/vhostmd/metric.dtd</filename>.</para>
      <para>For more information, see the manual pages <command>man 8 vhostmd</command> and <filename>/usr/share/doc/vhostmd/README</filename> on the VM Host Server system. On the guest, see the man page <command>man 1 vm-dump-metrics</command>.</para>
   </sect1>
</chapter>
